{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\lib\\site-packages (1.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\bsdor\\appdata\\roaming\\python\\python37\\site-packages (1.19.5)\n",
      "Requirement already satisfied: sklearn in c:\\programdata\\anaconda3\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\bsdor\\appdata\\roaming\\python\\python37\\site-packages (3.4.3)\n",
      "Requirement already satisfied: tensorly in c:\\programdata\\anaconda3\\lib\\site-packages (0.6.0)\n",
      "Requirement already satisfied: openpyxl in c:\\programdata\\anaconda3\\lib\\site-packages (3.0.7)\n",
      "Requirement already satisfied: keras in c:\\users\\bsdor\\appdata\\roaming\\python\\python37\\site-packages (2.6.0)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (1.7.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2.8.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2018.9)\n",
      "Requirement already satisfied: scikit-learn in c:\\programdata\\anaconda3\\lib\\site-packages (from sklearn) (0.22)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (2.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\bsdor\\appdata\\roaming\\python\\python37\\site-packages (from matplotlib) (8.3.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (1.0.1)\n",
      "Requirement already satisfied: nose in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorly) (1.3.7)\n",
      "Requirement already satisfied: et-xmlfile in c:\\programdata\\anaconda3\\lib\\site-packages (from openpyxl) (1.0.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\bsdor\\appdata\\roaming\\python\\python37\\site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.0.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\bsdor\\appdata\\roaming\\python\\python37\\site-packages (from kiwisolver>=1.0.1->matplotlib) (58.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas numpy sklearn matplotlib tensorly openpyxl keras scipy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\compat\\_optional.py:138: UserWarning: Pandas requires version '2.7.0' or newer of 'numexpr' (version '2.6.9' currently installed).\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA, FastICA, NMF\n",
    "import tensorly as tl\n",
    "from tensorly.decomposition import tucker,non_negative_tucker,parafac,non_negative_parafac\n",
    "from tensorly import tucker_to_tensor\n",
    "from tensorly.cp_tensor import cp_to_tensor\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "import scipy.integrate as integrate\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Machine_Learning_tutorials\\\\EMG_daven1.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-7aa6f7547016>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m## Import Data here\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mX\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Machine_Learning_tutorials\\EMG_daven1.xlsx'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Machine_Learning_tutorials\\trg.xlsx'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\excel\\_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[1;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols, storage_options)\u001b[0m\n\u001b[0;32m    362\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    363\u001b[0m         \u001b[0mshould_close\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 364\u001b[1;33m         \u001b[0mio\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    365\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    366\u001b[0m         raise ValueError(\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\excel\\_base.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, path_or_buffer, engine, storage_options)\u001b[0m\n\u001b[0;32m   1190\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1191\u001b[0m                 ext = inspect_excel_format(\n\u001b[1;32m-> 1192\u001b[1;33m                     \u001b[0mcontent_or_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1193\u001b[0m                 )\n\u001b[0;32m   1194\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mext\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\excel\\_base.py\u001b[0m in \u001b[0;36minspect_excel_format\u001b[1;34m(content_or_path, storage_options)\u001b[0m\n\u001b[0;32m   1069\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1070\u001b[0m     with get_handle(\n\u001b[1;32m-> 1071\u001b[1;33m         \u001b[0mcontent_or_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1072\u001b[0m     ) as handle:\n\u001b[0;32m   1073\u001b[0m         \u001b[0mstream\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    708\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    709\u001b[0m             \u001b[1;31m# Binary mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 710\u001b[1;33m             \u001b[0mhandle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    711\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    712\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Machine_Learning_tutorials\\\\EMG_daven1.xlsx'"
     ]
    }
   ],
   "source": [
    "## Import Data here\n",
    "X=pd.read_excel('Machine_Learning_tutorials\\EMG_daven1.xlsx')\n",
    "Y=pd.read_excel('Machine_Learning_tutorials\\trg.xlsx')\n",
    "print(X)\n",
    "print(Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Specify the model rank you want to investigate using this variable throughout\n",
    "num_components=3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##The task variables available here to predict are presented below:\n",
    "display(Y)\n",
    "\n",
    "##Select which variable you would like to predict by setting Type=1 for P1-P8 reaching tasks in \n",
    "##both forward and backwards directions, set Type=2 for the speed variable and Type=3 for the reaching diretion variable.\n",
    "\n",
    "Type=2\n",
    "\n",
    "if Type==1:\n",
    "    task=Y.iloc[:,0]\n",
    "elif Type==2:\n",
    "    task=Y.iloc[:,1]\n",
    "else:\n",
    "    task=Y.iloc[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##Non-negative matrix factorisation in both the spatial and temporal domains\n",
    "\n",
    "##https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html\n",
    "\n",
    "##Here are the hyperparameters for NNMF that you can adjust, press shift+tab while inside the function to see the options\n",
    "##in detail\n",
    "\n",
    "# 'init': The method to initialise the extracted components\n",
    "# 'solver': Numerical solver to use for the optimisation problem\n",
    "# 'beta_loss': the loss function\n",
    "# 'tol': the Tolerance of the stopping criterion\n",
    "# 'max_iter': Maximum number of iterations during timing out\n",
    "# 'alpha': a constant that adjust the influence of the regularisations terms in the loss function\n",
    "# 'l1_ratio': a mixing parameter that allows one to adjust the regularisation term towards either L1 or L2 penaly terms\n",
    "\n",
    "\n",
    "##What is the effect of adjusting each of these hyperparameters on the output???\n",
    "\n",
    "\n",
    "model=NMF(n_components=num_components,solver='cd',\n",
    "    beta_loss='frobenius',\n",
    "    tol=0.0001,\n",
    "    max_iter=200,\n",
    "    random_state=None,\n",
    "    alpha=0.0,\n",
    "    l1_ratio=0.0,\n",
    "    verbose=0,\n",
    "    shuffle=False)\n",
    "\n",
    "\n",
    "W_spatial = model.fit_transform(abs(X).T)\n",
    "H_spatial = model.components_\n",
    "\n",
    "pd.DataFrame(W_spatial).plot(kind='bar')\n",
    "plt.show()\n",
    "print('Timepoint specific activation coefficient :', H_spatial)\n",
    "\n",
    "X_temp=X.values.reshape(640, 50, 9).transpose(1,0,2).reshape(50,640*9)\n",
    "\n",
    "W_temporal=model.fit_transform(abs(X_temp))\n",
    "H_temporal = model.components_\n",
    "\n",
    "\n",
    "plt.plot(W_temporal)\n",
    "plt.show()\n",
    "print('Trial-specific muscle activation coefficient :', H_temporal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Principal component analysis\n",
    "## https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n",
    "\n",
    "\n",
    "##Try what you did regarding the hyperparameters here on PCA...\n",
    "\n",
    "model=PCA(n_components=num_components, copy=True, whiten=False, \n",
    "                          svd_solver='auto', tol=0.0, iterated_power='auto', random_state=None)\n",
    "\n",
    "model.fit(X)\n",
    "print('Explained variance ratio: ',model.explained_variance_ratio_)\n",
    "print('Singular values: ', model.singular_values_)\n",
    "\n",
    "X_pca=model.transform(X)\n",
    "\n",
    "plt.plot(X_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Independent component analysis\n",
    "##https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.FastICA.html\n",
    "\n",
    "model=FastICA(n_components=num_components, algorithm='parallel', whiten=True, \n",
    "        fun='logcosh', fun_args=None, max_iter=200, \n",
    "        tol=0.0001, w_init=None, random_state=None)\n",
    "model.fit(X)\n",
    "print('Components: ',model.components_)\n",
    "print('Mixing values: ', model.mixing_)\n",
    "print('Mean values: ', model.mean_)\n",
    "\n",
    "X_ica=model.transform(X)\n",
    "\n",
    "plt.plot(X_ica)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##Tensor decompositions: Tucker decomposition\n",
    "##http://tensorly.org/stable/user_guide/tensor_decomposition.html\n",
    "\n",
    "X_tensor=tl.tensor(abs(X).values.reshape(640,50,9))\n",
    "\n",
    "core, factors_tuck = tucker(X_tensor ,rank=[num_components,num_components,num_components])\n",
    "tucker_recon=tucker_to_tensor([core, factors_tuck])\n",
    "\n",
    "SSE=np.sum(np.linalg.norm((X_tensor-tucker_recon))**2)\n",
    "SST=np.sum(np.linalg.norm(X_tensor)**2)\n",
    "VAF_tuck=1-SSE/SST\n",
    "print('Variance accounted for: ',VAF_tuck)\n",
    "\n",
    "pd.DataFrame(factors_tuck[0]).plot()\n",
    "pd.DataFrame(factors_tuck[1]).plot()\n",
    "pd.DataFrame(factors_tuck[2]).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##Tensor decompositions: Non-negative Tucker\n",
    "##http://tensorly.org/stable/user_guide/tensor_decomposition.html\n",
    "X_tensor=tl.tensor(abs(X).values.reshape(640,50,9))\n",
    "core, factors_nnt =non_negative_tucker(X_tensor ,rank=[num_components,num_components,num_components])\n",
    "\n",
    "tucker_recon=tucker_to_tensor([core, factors_nnt])\n",
    "\n",
    "SSE=np.sum(np.linalg.norm((X_tensor-tucker_recon))**2)\n",
    "SST=np.sum(np.linalg.norm(X_tensor)**2)\n",
    "VAF_nnt=1-SSE/SST\n",
    "print('Variance accounted for: ',VAF_nnt)\n",
    "\n",
    "pd.DataFrame(factors_nnt[0]).plot()\n",
    "pd.DataFrame(factors_nnt[1]).plot()\n",
    "pd.DataFrame(factors_nnt[2]).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##Tensor decompositions: PARAFAC Decomposition\n",
    "##http://tensorly.org/stable/user_guide/tensor_decomposition.html\n",
    "X_tensor=tl.tensor(abs(X).values.reshape(640,50,9))\n",
    "factors_para = non_negative_parafac(X_tensor ,rank=num_components)\n",
    "parafac_recon=tl.cp_to_tensor(factors_para)\n",
    "SSE=np.sum(np.linalg.norm((X_tensor-parafac_recon))**2)\n",
    "SST=np.sum(np.linalg.norm(X_tensor)**2)\n",
    "VAF_para=1-SSE/SST\n",
    "print('Variance accounted for: ',VAF_para)\n",
    "\n",
    "pd.DataFrame(factors_para[1][0]).plot()\n",
    "pd.DataFrame(factors_para[1][1]).plot()\n",
    "pd.DataFrame(factors_para[1][2]).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Decode the task using the underlying parameters trial-specific coefficients from the above tensor decompositions\n",
    "## For the Task1-16 variable, a decoding accuracy >1/16 is significant and so on for the other variables.\n",
    "## https://www.frontiersin.org/articles/10.3389/fncom.2013.00008\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Tucker decomposition\n",
    "X_train, X_test, y_train, y_test=train_test_split(factors_tuck[0],task,train_size=0.85,stratify=task)\n",
    "\n",
    "model=LDA()\n",
    "model.fit(X_train,y_train)\n",
    "y_pred=model.predict(X_test)\n",
    "print('LDA Accuracy score: ',accuracy_score(y_test,y_pred))\n",
    "LDA_acc_tuck=accuracy_score(y_test,y_pred)\n",
    "plot_confusion_matrix(model,X_test,y_test)\n",
    "plt.show()\n",
    "\n",
    "#Naive-Bayes\n",
    "#https://scikit-learn.org/stable/modules/naive_bayes.html\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train,y_train)\n",
    "y_pred=gnb.predict(X_test)\n",
    "print('Naive Bayes Accuracy score: ',accuracy_score(y_test,y_pred))\n",
    "plot_confusion_matrix(gnb ,X_test,y_test)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "##K-Nearest Neighbors\n",
    "##https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestNeighbors.html\n",
    "knn=KNN(n_neighbors=5,weights='distance')\n",
    "knn.fit(X_train,y_train)\n",
    "y_pred=knn.predict(X_test)\n",
    "print('K-Nearest Neighbors Accuracy score: ',accuracy_score(y_test,y_pred))\n",
    "plot_confusion_matrix(knn ,X_test,y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##Non-negative Tucker decomposition\n",
    "X_train, X_test, y_train, y_test=train_test_split(factors_nnt[0],task,train_size=0.85,stratify=task)\n",
    "\n",
    "model=LDA()\n",
    "model.fit(X_train,y_train)\n",
    "y_pred=model.predict(X_test)\n",
    "print('LDA Accuracy score: ',accuracy_score(y_test,y_pred))\n",
    "LDA_acc_nnt=accuracy_score(y_test,y_pred)\n",
    "plot_confusion_matrix(model,X_test,y_test)\n",
    "plt.show()\n",
    "\n",
    "#Naive-Bayes\n",
    "#https://scikit-learn.org/stable/modules/naive_bayes.html\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train,y_train)\n",
    "y_pred=gnb.predict(X_test)\n",
    "print('Naive Bayes Accuracy score: ',accuracy_score(y_test,y_pred))\n",
    "plot_confusion_matrix(gnb ,X_test,y_test)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "##K-Nearest Neighbors\n",
    "##https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestNeighbors.html\n",
    "knn=KNN(n_neighbors=5,weights='distance')\n",
    "knn.fit(X_train,y_train)\n",
    "y_pred=knn.predict(X_test)\n",
    "print('K-Nearest Neighbors Accuracy score: ',accuracy_score(y_test,y_pred))\n",
    "plot_confusion_matrix(gnb ,X_test,y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##Parafac-decomposition\n",
    "X_train, X_test, y_train, y_test=train_test_split(factors_para[1][0],task,train_size=0.85,stratify=task)\n",
    "\n",
    "model=LDA()\n",
    "model.fit(X_train,y_train)\n",
    "y_pred=model.predict(X_test)\n",
    "print('LDA Accuracy score: ',accuracy_score(y_test,y_pred))\n",
    "LDA_acc_para=accuracy_score(y_test,y_pred)\n",
    "plot_confusion_matrix(model,X_test,y_test)\n",
    "plt.show()\n",
    "\n",
    "#Naive-Bayes\n",
    "#https://scikit-learn.org/stable/modules/naive_bayes.html\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train,y_train)\n",
    "y_pred=gnb.predict(X_test)\n",
    "print('Naive Bayes Accuracy score: ',accuracy_score(y_test,y_pred))\n",
    "plot_confusion_matrix(gnb ,X_test,y_test)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "##K-Nearest Neighbors\n",
    "##https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestNeighbors.html\n",
    "knn=KNN(n_neighbors=5,weights='distance')\n",
    "knn.fit(X_train,y_train)\n",
    "y_pred=knn.predict(X_test)\n",
    "print('K-Nearest Neighbors Accuracy score: ',accuracy_score(y_test,y_pred))\n",
    "plot_confusion_matrix(knn ,X_test,y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Can you find the model that gives the best output in terms of the VAF vs Task-decoding trade-off??\n",
    "## https://www.frontiersin.org/articles/10.3389/fncom.2013.00008\n",
    "\n",
    "print('Tucker decomposition: ', VAF_tuck*LDA_acc_tuck)\n",
    "print('Non-negative Tucker decomposition: ', VAF_nnt*LDA_acc_nnt)\n",
    "print('PARAFAC decomposition: ', VAF_para*LDA_acc_para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##Deep learning##\n",
    "\n",
    "##Below you can find the code setup for a multi-layer perceptron (MLP_, the simplest form of neural network.\n",
    "##Who's output layer is applicable to classification problems\n",
    "\n",
    "\n",
    "##Pick the output from a tensor decomposition to use as input into the MLP, set Type=1 for Tucker, 2 for Non-negative tucker\n",
    "##and 3 for PARAFAC. Note that the MLP is sensitive to the magnitude of the input variables and therefore it is necessary to\n",
    "##standardise them\n",
    "\n",
    "\n",
    "##Look across the web for a general rule for how many layers and nodes one should have in a neural network and see if it\n",
    "##produces the best results here.\n",
    "\n",
    "\n",
    "Type=1\n",
    "\n",
    "if Type==1:\n",
    "    X_mlp=factors_tuck[0]\n",
    "elif Type==2:\n",
    "    X_mlp=factors_nnt[0]\n",
    "else:\n",
    "    X_mlp=factors_para[1][0]\n",
    "\n",
    "    \n",
    "scaler= StandardScaler()\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test=train_test_split(X_mlp,task.values,train_size=0.85)\n",
    "\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "clf = MLPClassifier(solver='adam', alpha=1e-5,\n",
    "                     hidden_layer_sizes=(75,25), random_state=1)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_preds=clf.predict(X_test)\n",
    "print('MLP Accuracy score: ',accuracy_score(y_test,y_preds))\n",
    "plot_confusion_matrix(clf ,X_test,y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##Try to construct your own neural network that can classify the right reaching task...here is an example using the Keras framework\n",
    "##https://keras.io/\n",
    "\n",
    "##Pick the output from a tensor decomposition to use as input into the MLP, set Type=1 for Tucker, 2 for Non-negative tucker\n",
    "##and 3 for PARAFAC. Note that the neural network is sensitive to the magnitude of the input variables and therefore it is necessary to\n",
    "##standardise them\n",
    "\n",
    "Type=3\n",
    "\n",
    "if Type==1:\n",
    "    X_NN=factors_tuck[0]\n",
    "elif Type==2:\n",
    "    X_NN=factors_nnt[0]\n",
    "else:\n",
    "    X_NN=factors_para[1][0]\n",
    "\n",
    "    \n",
    "##This neural network cannot take multiple classes as a target variable, therefore we must encode the target variable\n",
    "##as a set of binary integers\n",
    "# encode class values as integers\n",
    "\n",
    "\n",
    "if task.name=='Task':\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(task)\n",
    "    encoded_Y = encoder.transform(task)\n",
    "    # convert integers to dummy variables (i.e. one hot encoded)\n",
    "    dummy_y = np_utils.to_categorical(encoded_Y)\n",
    "    X_train, X_test, y_train, y_test=train_test_split(X_NN,dummy_y,train_size=0.85)\n",
    "else:\n",
    "    X_train, X_test, y_train, y_test=train_test_split(X_NN,task.values,train_size=0.85)\n",
    "    \n",
    "##As with the MLP above, the neural network is sensitive to differences in magnitude between predictor variables that may not\n",
    "##necessarily be informative...therefore we will scale the input data accordingly\n",
    "scaler= StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##Here are some parameters you can adjust to optimise the neural network, how do these adjustments effect the output??:\n",
    "\n",
    "#The number of neurons in the layer\n",
    "#The number of layers\n",
    "#The activation function\n",
    "#The loss function\n",
    "#The optimizer\n",
    "#The metrics\n",
    "#The training epochs\n",
    "#The batch size\n",
    "#The number of cross-validations\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def NeuralNet_model():\n",
    "# create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(50, input_dim=num_components, activation='relu'))\n",
    "    #model.add(Dense(50, input_dim=num_components, activation='relu'))\n",
    "    model.add(Dense(np.unique(task)[-1], activation='softmax'))\n",
    "# Compile model\n",
    "    model.compile(loss='CategoricalCrossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.fit(X_train,y_train)\n",
    "    return model\n",
    "\n",
    "estimator = KerasClassifier(build_fn=NeuralNet_model, epochs=200, batch_size=1, verbose=0)\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "results = cross_val_score(estimator, X_test, y_test, cv=kfold)\n",
    "print(\"Cross validated score (Mean (Std)): %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
